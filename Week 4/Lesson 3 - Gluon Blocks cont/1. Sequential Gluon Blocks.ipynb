{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('MXNet-GluonCV')",
   "display_name": "Python 3.8.3 64-bit ('MXNet-GluonCV')",
   "metadata": {
    "interpreter": {
     "hash": "259aea3829f4adc8b9ea6739d853c3156d84ae46eaaaead38c32ebf6a00a2183"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "source": [
    "The purpose of the sequential class is to provide some useful convenience function to create sequential models. The sequential class itself is also a subclass of Gluon block and performs computation by simply chaining the composition of each layer one after each other. This allows us to define models consisting of many stacked layers in a very simple way."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = nn.Sequential()"
   ]
  },
  {
   "source": [
    "For example, let's try to reproduce a simple LeNet architecture. The LeNet architecture is named after a Yann LeCun.\n",
    "\n",
    "It is defined as follows:\n",
    "- We start with a convolutional layer with six channels of five-by-five canales,\n",
    "- A MaxPooling layer of pulling size two-by-two,\n",
    "- A 16 channels and five-by-five canales,\n",
    "- A MaxPooling layer of size two by two and\n",
    "- Then three fully-connected layer.\n",
    "    - One with a 120 unit and a tannage activation,\n",
    "    - Then one with 84 hidden units\n",
    "    - and one with 10 output units.\n",
    "\n",
    "We haven't specified the number of inputs for each of these layers because Gluon can automatically infer the input size for each layer when the network receives the first batch of data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2D(None -> 6, kernel_size=(5, 5), stride=(1, 1), Activation(tanh))\n",
       "  (1): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (2): Conv2D(None -> 16, kernel_size=(5, 5), stride=(1, 1), Activation(tanh))\n",
       "  (3): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
       "  (4): Dense(None -> 120, Activation(tanh))\n",
       "  (5): Dense(None -> 84, linear)\n",
       "  (6): Dense(None -> 10, linear)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "net.add(\n",
    "    nn.Conv2D(6, (5,5), activation='tanh'),\n",
    "    nn.MaxPool2D((2,2)),\n",
    "    nn.Conv2D(16, (5,5), activation='tanh'),\n",
    "    nn.MaxPool2D((2,2)),\n",
    "    nn.Dense(120, activation='tanh'),\n",
    "    nn.Dense(84),\n",
    "    nn.Dense(10)\n",
    ")\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.initialize()"
   ]
  },
  {
   "source": [
    "The sequential class inherets from the same base block class as the other layers and hence implements the callable interface by default. What this means in practice is that coding the net object with some data is equivalent to coding net `.forward` function with the same data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00731844  0.0057319   0.01617886 -0.00819516 -0.00975932 -0.02038096\n",
       "   0.00369545  0.00473428 -0.01314286  0.00593488]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "net(nd.ones((1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[ 0.00731844  0.0057319   0.01617886 -0.00819516 -0.00975932 -0.02038096\n",
       "   0.00369545  0.00473428 -0.01314286  0.00593488]]\n",
       "<NDArray 1x10 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "net.forward(nd.ones((1,1,28,28)))"
   ]
  },
  {
   "source": [
    "Here we passed NDArray of size one by one by 28 by 28 through the network, which is actually representative of a gray scale image of size 28 by 28 pixel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Another famous architecture that uses a simple sequential pattern is the VGG network. This architecture proved popular for image classification tasks. However, its large number of parameters made it impractical compared to smaller or more efficient new architectures like ResNet and Inception. However, this new architectures rely on complex data flows that cannot be encapsulated simply by chaining components together in a sequential fashion."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}