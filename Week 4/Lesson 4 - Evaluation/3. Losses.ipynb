{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Loss\n",
    "\n",
    "- We compute the loss by combining the output of the network and a corresponding ground truth label.\n",
    "- The loss is computed as a continuous and differentiable signal that is representative of how well our network is performing. Then, using this signal, we automatically update the value of our weights so that they will get better at lowering this loss value at the next iteration.\n",
    "- We repeat this process thousands or even millions of times until these weights have converged to a local minima of the loss function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon import loss\n",
    "import numpy as np"
   ]
  },
  {
   "source": [
    "loss functions are chosen such that they are minimized when the predictions are perfect, but have high values as the predictions get worse. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0.01, 1, 0.001)\n",
    "plt.plot(x, -np.log(x))\n",
    "plt.title('Log Loss when True Label = 1')\n",
    "plt.xlabel('Predicted probability')\n",
    "o = plt.ylabel('loss')"
   ]
  },
  {
   "source": [
    "The loss function first normalizes the raw output of your network using the Softmax activation. The Softmax activation normalizes each value between 0 and 1, and makes sure that they all sum up to 1.\n",
    "\n",
    "$$\\text{Softmax}(x_i)=\\frac{e_i^x}{\\sum_{j=1}^Ne_j^x}$$\n",
    "\n",
    "This effectively turns the normalized output in a probability distribution over all your classes. You can see in the below example that the values are indeed normalized, and all sum up to 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nd.array([[1,2,3,4,5]]).softmax()"
   ]
  },
  {
   "source": [
    "nd.array([[1,2,3,4,5]]).softmax().sum()"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Now we take the Softmax cross entropy loss class from the gluon loss package. This function computes a Softmax cross entropy loss using your model outputs and the true label."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = nd.array([[10,20,40,5]])\n",
    "label = nd.array([1])\n",
    "\n",
    "loss_fn(predictions, label)"
   ]
  },
  {
   "source": [
    "We can see here that the highest activation is for\n",
    "the item at index 2 here, with a value of 40.\n",
    "Meanwhile, the label is set to correspond to index 1.\n",
    "Unsurprisingly, the loss is pretty high.\n",
    "When we compute it here, we get a loss of 20. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now if we change the label to be set at index 2,\n",
    "which corresponds to the highest activation,\n",
    "we can see that the loss is now 0, which is what we would expect.\n",
    "Indeed, prediction is correct in this case. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = nd.array([2])\n",
    "loss_fn(predictions, label)"
   ]
  },
  {
   "source": [
    "We want the loss to be representative of how well our network is doing and\n",
    "how accurate its predictions are.\n",
    "There are a variety of different losses for different tasks.\n",
    "A lot of neural network training recipes actually use a combination of losses\n",
    "during training in order to optimize in parallel for multiple objectives.\n",
    "For example, in the object detection task, we optimize for\n",
    "combination of the localization loss,\n",
    "to learn to correctly detect the position of the bounding boxes.\n",
    "And the object classification loss,\n",
    "to learn to classify which objects are in each of the detected boxes. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}