{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.9.0 64-bit ('MXNet-GluonCV-WSL': venv)",
   "display_name": "Python 3.9.0 64-bit ('MXNet-GluonCV-WSL': venv)",
   "metadata": {
    "interpreter": {
     "hash": "3023c2e62892d3e732034fcc083e0f86f0106296626c1389b94e170cf50df5a6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Accuracy\n",
    "Metrics, such as Accuracy, calculate scores from multiple batches. You don't need to keep track of all the predictions and labels."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import metric\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metric.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('accuracy', 1.0)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Correct predictions\n",
    "labels = nd.array([1,2,3])\n",
    "predections = nd.array([1,2,3])\n",
    "accuracy.update(labels,predections)\n",
    "accuracy.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('accuracy', 0.3333333333333333)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Erroneous predictions\n",
    "accuracy.reset()\n",
    "predections = nd.array([1,1,1])\n",
    "accuracy.update(labels,predections)\n",
    "accuracy.get()"
   ]
  },
  {
   "source": [
    "## Multiclass Classification"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('accuracy', 0.0)\n('top_k_accuracy_2', 1.0)\n"
     ]
    }
   ],
   "source": [
    "# TopK Accuracy\n",
    "top_2_accuracy = metric.TopKAccuracy(top_k=2)\n",
    "predections = nd.array([[0.4, 0.5, 0.1],[0.4, 0.5, 0.1],[0.4, 0.5, 0.1]])\n",
    "labels = nd.array([0,0,0])\n",
    "\n",
    "accuracy.reset()\n",
    "accuracy.update(labels, predections)\n",
    "print(accuracy.get())\n",
    "\n",
    "top_2_accuracy.update(labels, predections)\n",
    "print(top_2_accuracy.get())"
   ]
  },
  {
   "source": [
    "We can also have metrics that are specific to certain tasks and domain as well.\n",
    "\n",
    "For example, in object detection, the common metric is the precision at intersection of our unions threshold.\n",
    "\n",
    "When we say precision at 0.5 for an object detection task, we mean that any correctly labeled class with more than 0.5 IOU, intersection of a union, overlap with a ground truth bounding box is considered a correct prediction."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "$$\\text{IoU} = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}