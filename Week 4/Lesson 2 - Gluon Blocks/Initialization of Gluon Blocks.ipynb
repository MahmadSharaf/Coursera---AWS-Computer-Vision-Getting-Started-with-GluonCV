{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.3 64-bit ('MXNet-GlounCV')",
   "display_name": "Python 3.8.3 64-bit ('MXNet-GlounCV')",
   "metadata": {
    "interpreter": {
     "hash": "e70749184dd998f0a32c9cabe795e8b7675f08ddb47f65e88acad92efc19cb23"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Initialization\n",
    "Initialization is simply what values we set the parameters of the layers to, when we create the network. Several papers have shown that the initialization method of a network's parameters greatly influence the speed and the ability of the network to train properly and convert."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import init, nd\n",
    "from mxnet.gluon import nn"
   ]
  },
  {
   "source": [
    "### Different types of initialization\n",
    "- By default, MXNet initializes the weight matrices uniformly, by drawing from a uniform distribution between -0.07\n",
    "and 0.07. And the bias parameters are all set to zero.\n",
    "- However, we often need to use other methods to initialize weights.\n",
    "- MXNet provides support for a lot of different initialization schemes from simple constant initialization, like one or zero to randomly uniform weights. And more complex ones like Xavier initialization that takes into account the number of inputs and outputs of a given layer to scale the weights' magnitude. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple\n",
    "init.One\n",
    "init.Zero\n",
    "# Randomly uniform weights\n",
    "init.Uniform\n",
    "init.Normal\n",
    "init.Xavier\n",
    "init.Constant"
   ]
  },
  {
   "source": [
    "Xavier Initialization, a very popular initialization scheme for training neural networks. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[[[ 0.05636501  0.10720772  0.24847925]\n",
       "   [ 0.39752382  0.11866093  0.41332   ]\n",
       "   [ 0.05182666  0.4009717  -0.08815584]]]]\n",
       "<NDArray 1x1x3x3 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# creates a Conv2D layer with one input channel and one output channel and kernel size three by three.\n",
    "layer = nn.Conv2D(channels = 1, in_channels=1, kernel_size=(3,3))\n",
    "layer.initialize(init.Xavier())\n",
    "layer.weight.data()"
   ]
  },
  {
   "source": [
    "Xavier Initialization large number of output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[[ 0.00630558  0.00744513 -0.00590012]\n",
       "  [-0.00318499 -0.01033202  0.01999258]\n",
       "  [-0.0226214   0.02366119 -0.01160159]]\n",
       "\n",
       " [[-0.0059481  -0.00113977  0.01488703]\n",
       "  [ 0.01593029  0.00147454 -0.00102179]\n",
       "  [ 0.00347238 -0.0054713   0.02171864]]\n",
       "\n",
       " [[ 0.01715045 -0.02189048 -0.00829784]\n",
       "  [-0.02106922  0.00756137 -0.02448375]\n",
       "  [-0.00672377  0.01697394  0.0233291 ]]\n",
       "\n",
       " ...\n",
       "\n",
       " [[-0.01893761  0.01284147 -0.00975751]\n",
       "  [-0.02142992 -0.00148998 -0.00093406]\n",
       "  [ 0.00985651 -0.00277236  0.00710726]]\n",
       "\n",
       " [[ 0.00880146 -0.01043105 -0.00261599]\n",
       "  [-0.00773894  0.01042632 -0.01882804]\n",
       "  [ 0.00926955  0.00075613  0.01006069]]\n",
       "\n",
       " [[ 0.00549796  0.00605232 -0.0150173 ]\n",
       "  [-0.01781082  0.02513626  0.01330902]\n",
       "  [-0.01826004  0.01434206  0.0111638 ]]]\n",
       "<NDArray 512x3x3 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Increase the number of input and output channels\n",
    "layer = nn.Conv2D(channels = 512, in_channels=512, kernel_size=(3,3))\n",
    "layer.initialize(init.Xavier())\n",
    "layer.weight.data()[0]"
   ]
  },
  {
   "source": [
    "### Deferred Initialization\n",
    "- This mechanism allows you to only define the number of outputs for each layer. And the number of input is completed automatically during the first pass of the data through that layer."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter conv2_weight (shape=(1, 0, 3, 3), dtype=<class 'numpy.float32'>)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# create a layer without defining the number of input channels.\n",
    "layer = nn.Conv2D(channels=1, kernel_size=(3,3))\n",
    "layer.weight"
   ]
  },
  {
   "source": [
    "we can see that parameter of shape is 1, 0, 3, 3. This means that we still don't know how many inputs this conditional layer will accept, and hence, we don't know the value of the depth dimension of the kernel."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "If we run a batch of data through the layer of size 1 by 8 by 224 by 224, like this, the layer detects that the input has 8 channels and initialize the weights according to the initialization rule and with the right shape."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Parameter conv2_weight (shape=(1, 8, 3, 3), dtype=<class 'numpy.float32'>)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "layer.initialize(init.Xavier())\n",
    "layer(mx.nd.ones((1,8,224,224)))\n",
    "layer.weight"
   ]
  },
  {
   "source": [
    "### Initialization Context\n",
    "Very important concept in gluon, if that innate work is on a specific compute context. If the network weights were presented as in the arrays on the compute context, then the network is said to be on that context. To give you an example, if you initialize all the ways of a network on one GPU, then that means that your network is on that GPU. You can then run inference by running data located on the same compute context, CPU or GPU as your network weights and the output will be on that compute context as well. For example, here we can see that the outputs of this Conv2D layer after having data allocated on the GPU, the output of this layer is also on GPU."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Conv2D(channels=1, kernel_size=(3,3))\n",
    "layer.initialize(init.Xavier(), ctx=mx.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[[[-0.40201202 -0.7018695   0.01785277  0.5870792   0.2730119\n",
       "    -0.28182274 -0.04584998]\n",
       "   [-0.2916031  -0.528964   -0.29903355  0.07407004  0.0802862\n",
       "     0.3445815  -0.13694511]\n",
       "   [-0.59821695 -0.02556936  0.23767231 -0.13543423 -0.5430629\n",
       "    -0.27945024 -0.20130056]\n",
       "   [-0.30110553 -0.65429443 -0.60643345 -0.1521534   0.6870324\n",
       "     0.33310294  0.12388384]\n",
       "   [-0.5037831   0.38428587  0.40459839  0.13650624 -0.41439134\n",
       "    -0.16389096 -0.5943488 ]\n",
       "   [-0.3006251  -0.0619027   0.0304675  -1.034457   -0.04781772\n",
       "     0.00905298 -0.41354978]\n",
       "   [-0.48301694 -0.08969672  0.18813546  0.59333855  0.35146442\n",
       "     0.1964373  -0.61889297]]]]\n",
       "<NDArray 1x1x7x7 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "layer(nd.uniform(shape=(1,3,9,9), ctx=mx.cpu()))"
   ]
  },
  {
   "source": [
    "### Set Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight.set_data(nd.ones((1,3,3,3), ctx=mx.gpu()))\n",
    "layer.weight.data()"
   ]
  }
 ]
}