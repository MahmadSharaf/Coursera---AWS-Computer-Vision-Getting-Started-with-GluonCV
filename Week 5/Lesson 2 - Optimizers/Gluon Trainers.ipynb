{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import nd, autograd, optimizer, gluon\n",
    "\n",
    "# The optimizer library contains various mx and optimizers that are implementations of popular and state of the art deep learning optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize a simple model\n",
    "net = gluon.nn.Dense(1)\n",
    "net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before using the trainer to update model parameters, forward and backward passes must be run first\n",
    "\n",
    "# The dataset has eight samples with four features\n",
    "batch_size = 8\n",
    "X = nd.random.uniform(shape=(batch_size, 4))\n",
    "y = nd.random.uniform(shape=(batch_size,))\n",
    "\n",
    "loss = gluon.loss.L2Loss()\n",
    "\n",
    "def forward_backward():\n",
    "    with autograd.record():\n",
    "        l = loss(net(X), y)\n",
    "    l.backward()\n",
    "\n",
    "forward_backward()"
   ]
  },
  {
   "source": [
    "Now, let's create a trainer instance using the model parameters and the simple optimizer stochastic gradient descent with learning rate as 1.\n",
    "\n",
    "When creating an gluon trainer you must provide a collection of parameters that need to be learned.\n",
    "- To get all of the trainable parameters from a gluon network or block you can invoke the `collect_params' method of the network, it returns a parameter dictionary. Our trainer uses these to access the parameters that need to be updated.\n",
    "- You also provide an `optimizer` that will be used to calculate new values of parameters every training iteration.\n",
    "- You can also specify the hyperparameters for the optimizer using `optimizer_params`."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainer instance using the model parameter and the simple optimizer stochastic gradient descent with learning rate as 1\n",
    "\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer='sgd', optimizer_params={'learning_rate':1})"
   ]
  },
  {
   "source": [
    "Before updating, let's check the current network parameters. We can do that by accessing the data in the `weight` field of the block."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[[ 0.06700657 -0.00369488  0.0418822   0.0421275 ]]\n<NDArray 1x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "curr_weight = net.weight.data().copy()\n",
    "print(curr_weight)"
   ]
  },
  {
   "source": [
    "### Trainer Step"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now, we will call the `step` method to perform one update.\n",
    "\n",
    "We provide the batch size as an argument to normalize the size of the gradient and make it independent of the batch. Otherwise, we'd get larger gradients with larger batch sizes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[[0.26870278 0.2342061  0.444427   0.33788317]]\n<NDArray 1x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "trainer.step(batch_size)\n",
    "print(net.weight.data())"
   ]
  },
  {
   "source": [
    "We can see the network parameters have now changed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Results of SGD update\n",
    "\n",
    "Since we use spleen SGD the update rule is the old weight minus the learning rate times the gradient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n[[0.26870278 0.2342061  0.444427   0.33788317]]\n<NDArray 1x4 @cpu(0)>\n"
     ]
    }
   ],
   "source": [
    "# We can verify our trainer step by running the following code snippet which is explicitly performing the SGD update.\n",
    "print(curr_weight - net.weight.grad() * 1 / batch_size)"
   ]
  },
  {
   "source": [
    "The values are identical to the result of trainer dot step earlier."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Using Optimizer Instance\n",
    "\n",
    "we can also pass an optimizer instance directly into the trainer constructor."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this case we'll use the Adam Optimizer the popular adaptive optimizer for deep learning. In MXNet you can simply call `optimizer.Adam` and pass in the learning rate. We can initialize the gluon trainer as before with the network parameters. But now we'll pass the optimizer objects directly as an argument."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = optimizer.Adam(learning_rate=1)\n",
    "trainer = gluon.Trainer(net.collect_params(), optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[-0.73130214 -0.7657989  -0.5555788  -0.6621224 ]]\n",
       "<NDArray 1x4 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "forward_backward()\n",
    "trainer.step(batch_size)\n",
    "net.weight.data()"
   ]
  },
  {
   "source": [
    "### Changing the Learning Rate\n",
    "\n",
    "sometimes we may need to change the learning rate during training."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Accessing the current Learning Rate\n",
    "trainer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Changing the Learning Rate\n",
    "trainer.set_learning_rate(0.1)\n",
    "trainer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(),\n",
    "                        'adam',\n",
    "                        optimizer_params={'learning_rate':0.001})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    loss = loss(net(X), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autograd.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.step(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}