{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "We can use the `autograd` package from `MXNet` to calculate gradients of arbitrary functions. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import autograd"
   ]
  },
  {
   "source": [
    "As a toy example, let's say we're interested in differentiating a function $f(x) = 2x^2$ with respect to parameter $x$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[[1. 2.]\n",
       " [3. 4.]]\n",
       "<NDArray 2x2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Assigning an initial value to x\n",
    "x = nd.array([[1,2],[3,4]])\n",
    "x"
   ]
  },
  {
   "source": [
    "While computing the gradient of $f(x)$ with respect to $x$, you will need a place to store and retrieve the gradient. For a particular NDArray, you need to use attach grad to signal that you're going to compute the gradient of a function with respect to that NDArray.\n",
    "\n",
    "In this example, we're calling `attach_grad` on `x` because we want to compute the gradient of $f$ with respect to $x$."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.attach_grad()"
   ]
  },
  {
   "source": [
    "We also need to allocate space to store the gradient after it's computed and attach grad takes care of this as well."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now, let's evaluate the function $y=f(x)$.\n",
    "\n",
    "First, we need to define the function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * x**2"
   ]
  },
  {
   "source": [
    "As long as the argument `x` is passed in as an NDArray and we keep the objects as NDArray is in the function, `autograd` we'll be able to compute the gradient."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with autograd.record():\n",
    "    y = f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(\n",
       " [[1. 2.]\n",
       "  [3. 4.]]\n",
       " <NDArray 2x2 @cpu(0)>,\n",
       " \n",
       " [[ 2.  8.]\n",
       "  [18. 32.]]\n",
       " <NDArray 2x2 @cpu(0)>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "x, y"
   ]
  },
  {
   "source": [
    "As you can see, the contents of `y` are the results of evaluating $2x^2$. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now, to compute the gradient you will have to call the backward function on y. This will invoke backpropagation on y and compute gradient of y with respect to all its upstream computational dependencies. It will store the gradient in the space allocated by the attach grad function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "source": [
    "Now, we can verify the computer gradients to see if it's correct.\n",
    "\n",
    "The analytical derivative of $f(x)=2x^2$ is $4x$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(\n",
       " [[1. 2.]\n",
       "  [3. 4.]]\n",
       " <NDArray 2x2 @cpu(0)>,\n",
       " \n",
       " [[ 4.  8.]\n",
       "  [12. 16.]]\n",
       " <NDArray 2x2 @cpu(0)>)"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "x, x.grad"
   ]
  },
  {
   "source": [
    "The gradient computed on an NDArray that you call attach grad on, will be stored in the grad property of x."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Dynamic programs\n",
    "\n",
    "Sometimes it's necessary to write dynamic programs where the execution flow depends on values computed in real-time as part of the execution."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "MXNet will record the execution trees and compute the gradient as well. \n",
    "\n",
    "Consider the following function f. \n",
    "- It takes in an input vector of size two each drawn randomly from the uniform distribution on negative one to one.\n",
    "- F doubles the input vector until its norm or length which is 1,000. \n",
    "- Then it selects one element depending on the sum of its elements. If it's positive, it returns the first element and the second element if it's negative."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    x = x*2\n",
    "    while x.norm().asscalar() < 1000:\n",
    "        x = x*2\n",
    "        # If sum positive\n",
    "        # Pick 1st\n",
    "    if x.sum() >= 0:\n",
    "        y = x[0]\n",
    "    # else pick 2nd\n",
    "    else:\n",
    "        y = x[1]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[0.09762704 0.18568921]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# First, we initialize x with two numbers drawn randomly from a uniform distribution between minus one and one.\n",
    "x = nd.random.uniform(-1, 1, shape=2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach grad to x and record the trace of the function evaluation before calling backwards to compute the gradients.\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "source": [
    "How to evaluate these gradients?\n",
    "\n",
    "$$y =k.x[0] \\text{ or } y=k.x[1], \\text{ hence } \\dfrac{\\mathrm{d}y}{\\mathrm{d}x} = \\begin{vmatrix} 0 \\\\ k \\end{vmatrix} \\text{ or } \\begin{vmatrix} k \\\\ 0 \\end{vmatrix}$$\n",
    "\n",
    "$$\\text{with } k = 2^n \\text{where} n \\text{is the number of times } x \\text{was multiplied by 2}$$\n",
    "\n",
    "Breaking it down, we know that $y$ is a linear function of one of the items in $x$. If we represent the coefficient in the linear function as $k$, then the gradient with respect to $x$ will either be $k$, 0 or 0, $k$ depending on which element from $x$ is picked. We also know what $K$ is, it is the number of times we doubled $x$ in the loop. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[0.09762704 0.18568921]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\n",
       "[8192.    0.]\n",
       "<NDArray 2 @cpu(0)>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "x.grad"
   ]
  }
 ]
}